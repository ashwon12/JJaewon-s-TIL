# 10월 19일 TIL

## 웹 스크래핑(크롤링)

크롤링 : 우리가 원하는 것들의 데이터를 구글 검색 엔진들이 모든 웹사이트들을 다 가져오는 것

### 사용하기!

- `requests` 라이브러리

```jsx
headers = {'User-Agent' : 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)AppleWebKit/537.36 (KHTML, like Gecko) Chrome/73.0.3683.86 Safari/537.36'}
data = requests.get('https://movie.naver.com/movie/sdb/rank/rmovie.nhn?sel=pnt&date=20200716',headers=headers)
```

원하고자 하는 데이터가있는 곳의 url을 입력해서 GET 방식으로 요청을 보낸다. 

headers `User-Agent`라는 값을 넣어줘야 보내주는 얘들이 뭔지 알아서 보내준다. 

동기방식으로 진행된다. 그래서 조금 느리다.

<br>

- `BeautifulSoup` 라이브러리

```jsx
soup = BeautifulSoup(data.text, 'html.parser')

soup.select('body > #') # 셀렉터의 값을 배열로 반환
soup.selectone('body > #')   # 하나만 가져다줌, 그래서 배열아니라 그냥 값

print(tr_list) # 속성을 가져올 땐 딕셔너리처럼 키를 [] 안에 적어주면 된다.
```

css의 class를 선택해주듯이 웹 페이지에서 내가 원하는 부분의 데이터만 가져올 수 있게 도와주는 라이브러리

파이썬은 html을 모르기 때문에 얘가 알 수 있게 도와준다. == 파싱

CSS 셀렉터 문법 그대로 사용 

<br>

### **예외**

ajax는 웹사이트가 나온 후 별도로 요청을 해서 받아오기 때문에 스크래핑이 안된다.

⇒ 처음에 나와있는 정보가 아닌 것들은 스크래핑이 불가하다.

**해결방법**

1. 콘솔 → network → 요청한 url 확인하기 → url에 들어가면 api 등장!

    → url 규칙 파악 

2. selenium - 브라우저 제어

너가 로그인했을 때 나오는 화면 가져와!!!!! 내가 할 수 있는 것들을 전부다 자동화 설정해 놓는 것, 하지만 조금 느리다,,,,

속도는 request가 가장 빠르기 때문에 이걸 우선적으로 쓰고 셀레니엄이 다 할 수 있어서 request랑 셀레니엄이랑 섞어서 병렬처럼 많이들 사용한다.

<br>

### 크롤링을 해도 되는지 확인하는 방법

url + `robots.txt` 해서 안에 들어있는 내용이 가능하다는 내용인지 확인

<br><br>


## 한줄평

안드로이드 공기계연결로 하루를 날리고,,, soscon 보고서 작성하고 오픈소스 주제선택하고,,,과제하면서 며칠동안 TIL을 작성을 못했다,,, 반성해라!! ㅠㅠ

오늘 크롤링 배웠는데 너무너무 재밌었다. 셀레니엄은 자세하게 안배웠는데 크롤링+셀레니엄 복습 많이 해야겠다!
